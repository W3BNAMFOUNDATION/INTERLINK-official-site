<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>INTERLINK‚Ñ¢ ‚Äî Documentation</title>
    <link rel="stylesheet" href="assets/styles/style.css">
</head>

<body>

<section class="hero">
    <h1>INTERLINK‚Ñ¢ Documentation</h1>
    <p>All official technical, architectural, security, and integration documentation.</p>
</section>

<section class="container">

    <h2>üìò System Architecture</h2>
    <p>Complete overview of INTERLINK‚Äôs backend, memory engine, ingest modules, filesystem restrictions, and the Gibberlink‚Ñ¢ protocol.</p>

    <ul>
        <li><strong>Core Engine Overview</strong></li>
        <li><strong>Backend Routing Architecture</strong></li>
        <li><strong>Memory Vector Engine</strong></li>
        <li><strong>Ingest System Design</strong></li>
        <li><strong>Filesystem Safety Model</strong></li>
        <li><strong>Gibberlink‚Ñ¢ Distributed Pipe</strong></li>
    </ul>

    <h2>üß† Memory Engine</h2>
    <p>INTERLINK incorporates a distributed vector memory engine optimized for reasoning and multimodal ingest.</p>

    <ul>
        <li>FAISS-based vector embeddings</li>
        <li>Long-context stitching</li>
        <li>Distributed memory cells</li>
        <li>Fast semantic search</li>
        <li>PDF / Web / Audio memory ingestion</li>
    </ul>

    <h2>üì° Ingest Documentation</h2>
    <p>Supported ingest formats:</p>

    <ul>
        <li>PDF (via PDFPlumber)</li>
        <li>YouTube (via PyTube or Whisper)</li>
        <li>Web URLs (via BS4 + Requests)</li>
        <li>Local text ingestion</li>
        <li>Audio transcription</li>
    </ul>

    <h3>CLI Commands</h3>

<pre><code>
python w3.py ingest pdf path/to/file.pdf
python w3.py ingest url https://example.com
python w3.py ingest yt https://youtube.com/...
python w3.py chat groq "Explain quantum finance"
</code></pre>

    <h2>üîê Security Framework</h2>

    <ul>
        <li>Zero-Fault Filesystem Model</li>
        <li>Sandboxed ingest environment</li>
        <li>API key isolation (no hardcoded keys)</li>
        <li>Service profile separation</li>
        <li>WebSocket safety</li>
    </ul>

    <h2>üîå API Reference</h2>

    <p>INTERLINK exposes a clean API for any external system:</p>

<pre><code>
POST /api/send
{
  "service": "groq",
  "prompt": "Hello world"
}
</code></pre>

    <h2>‚öôÔ∏è Service Profiles</h2>
    <p>All models and endpoints are configured in:</p>

<pre><code>config/service_profiles.yaml</code></pre>

    <p>Supported profiles:</p>

    <ul>
        <li>Ollama (local)</li>
        <li>DeepSeek (API)</li>
        <li>Groq (API)</li>
        <li>OpenAI-compatible models</li>
    </ul>

    <h2>üöÄ Deployment</h2>

    <ul>
        <li>Local Dev Deployment</li>
        <li>Cloud VM Deployment</li>
        <li>Linux Server Deployment</li>
        <li>Dockerized Deployment (optional)</li>
    </ul>

    <h2>üìà Performance</h2>

    <ul>
        <li>Fast boot time</li>
        <li>Parallel ingest threads</li>
        <li>Vector cache pre-loading</li>
        <li>Low-latency inference routing</li>
    </ul>

    <h2>üîÑ Updating the Engine</h2>

<pre><code>
git pull origin main
python install.py
</code></pre>

    <h2>üìû Support</h2>
    <p>Email: support@interlink.ai</p>

</section>

<script src="assets/scripts/ui.js"></script>
<script src="assets/scripts/script.js"></script>
</body>
</html>
